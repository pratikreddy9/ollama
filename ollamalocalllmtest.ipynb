{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"ingredients\": {\\n    \"bread\": 2,\\n    \"cheese\": 1,\\n    \"meat\": 1,\\n    \"tomatoes\": 2,\\n    \"lettuce\": 1\\n  },\\n  \"instructions\": [\\n    \"Spread cheese on one slice of bread.\",\\n    \"Top with meat, tomatoes, and lettuce.\",\\n    \"Close the sandwich with the other slice of bread.\",\\n    \"Cut the sandwich in half and enjoy!\"\\n  ]\\n}'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#json mode ollama text models\n",
    "import ollama\n",
    "\n",
    "system_prompt = \"Respond using JSON\"\n",
    "user_prompt = \"sandwhich recipee\"\n",
    "expected_output = \"\"\n",
    "modelid = \"gemma:7b\"\n",
    "\n",
    "def ollamajson(system_prompt,user_prompt,expected_output,modelid):\n",
    "  \n",
    "  response = ollama.chat(model=modelid, messages=\n",
    "                         [\n",
    "                           {\"role\": \"system\",\"content\": f\"system prompt : {system_prompt}\",},\n",
    "                           {'role': 'user','content':f\"user prompt : {user_prompt}\",},\n",
    "                           {'role': 'user','content':f\"expected output : {expected_output}\",},\n",
    "                           ],\n",
    "                             format=\"json\")\n",
    "  \n",
    "  content = response.get(\"message\", {}).get(\"content\", \"{}\")\n",
    "  return content\n",
    "\n",
    "ollamajson(system_prompt,user_prompt,expected_output,modelid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"recipe\": \"Classic Ham and Cheese Sandwich Recipe\",\\n\"ingredients\": [\"White bread, sturdy enough for toasting without disintegrating\",\"Sliced deli ham\",\"Sharp cheddar cheese slices or shredded\",\"Butter (for spreading on the outer layer of bread)\", \"Mayonnaise (optional, adds creaminess)\"],\\n\"instructions\": [\"Preheat a pan to medium heat.\", \"Spread butter evenly on one side of two slices of white bread.\", \"Place the mayonnaise onto another slice if desired for extra richness and help prevent sogginess after refrigeration (optional).\", \"Lay out your ham, cheese slices or shredded, overlying them in a single layer to avoid tearing when spreading on top of each other.\", \"Place the second buttered side down onto one slice if using mayonnaise and cover with another bread slice, creating an assembled sandwich.\", \"Cook for 2-3 minutes per side or until golden brown. You can also use a grill pan to achieve similar results with charred flavor notes.\"]}'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this response is a json mode script tht is using microsft phi 3 model\n",
    "import ollama\n",
    "\n",
    "system_prompt = \"Respond using JSON\"\n",
    "user_prompt = \"sandwhich recipee\"\n",
    "expected_output = \"\"\n",
    "modelid = \"phi3:3.8b\"\n",
    "\n",
    "def ollamajson(system_prompt,user_prompt,expected_output,modelid):\n",
    "  \n",
    "  response = ollama.chat(model=modelid, messages=\n",
    "                         [\n",
    "                           {\"role\": \"system\",\"content\": f\"system prompt : {system_prompt}\",},\n",
    "                           {'role': 'user','content':f\"user prompt : {user_prompt}\",},\n",
    "                           {'role': 'user','content':f\"expected output : {expected_output}\",},\n",
    "                           ],\n",
    "                             format=\"json\")\n",
    "  \n",
    "  content = response.get(\"message\", {}).get(\"content\", \"{}\")\n",
    "  return content\n",
    "\n",
    "ollamajson(system_prompt,user_prompt,expected_output,modelid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NPWP\n"
     ]
    }
   ],
   "source": [
    "#this script is for vision models ie moondream\n",
    "\n",
    "import ollama\n",
    "\n",
    "\n",
    "prompt = \"\"\"extract all data you see in these images in a proper json format and make sure to keep the keys in english no matter what the language is     Example format for KTP:\n",
    "    {\n",
    "        \"NIK\": \"string\",\n",
    "        \"Name\": \"string\",\n",
    "        \"Place/Date of Birth\": \"string\",\n",
    "        \"Gender\": \"string\",\n",
    "        \"Address\": \"string\",\n",
    "        \"RT/RW\": \"string\",\n",
    "        \"Village/District\": \"string\",\n",
    "        \"Sub-district\": \"string\",\n",
    "        \"Religion\": \"string\",\n",
    "        \"Marital Status\": \"string\",\n",
    "        \"Occupation\": \"string\",\n",
    "        \"Nationality\": \"string\",\n",
    "        \"Valid Until\": \"string\"\n",
    "    }\n",
    "\n",
    "    Example format for NPWP:\n",
    "    {\n",
    "        \"NPWP\": \"string\",\n",
    "        \"Name\": \"string\",\n",
    "        \"Address\": \"string\",\n",
    "        \"Registered Date\": \"string\",\n",
    "        \"Note\": \"string\"\n",
    "    }\"\"\"\n",
    "\n",
    "res = ollama.chat(\n",
    "    model=\"moondream\",\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f'{prompt}',\n",
    "            'images': ['/Users/p/Desktop/work/ayotta/NPWP IRFAN AWALUDIN.jpg']\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(res['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason why the sky appears to be blue is due to various atmospheric phenomena that occur when sunlight passes through the atmosphere. \n",
      "\n",
      "1. Reflection: The Earth, which contains most of our atmosphere, absorbs blue light from the sun and reflects red light back into space. This means that as sunlight travels through the atmosphere, some of it is absorbed by clouds, water vapor, and other particles in the air, causing some of the visible blue light to be reflected off these surfaces.\n",
      "\n",
      "2. Blue Light: The remaining light of the sun that does not get reflected is called blue light. In our sky, we observe this as a beautiful, soft blue color. This blue light from the sun also absorbs less red light than other wavelengths, creating an illusion of blue.\n",
      "\n",
      "3. Red and Violet Light: As sunlight passes through clouds, it is scattered and filtered by these particles, some of which are absorbed in the atmosphere, while others scatter and return to space. The visible light spectrum can be divided into three main colors:\n",
      "\n",
      "   a) Blue light (blue color)\n",
      "   b) Red light (red color)\n",
      "   c) Green light\n",
      "\n",
      "The sky appears blue because it is due to the reflection and scattering of blue light by clouds in our atmosphere, which absorbs more blue light than red or green light. This phenomenon was discovered experimentally by Heinrich Hertz in 1864."
     ]
    }
   ],
   "source": [
    "#this script is for streaming responses.\n",
    "\n",
    "import ollama\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model='qwen2:0.5b',\n",
    "    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One fascinating and lesser-known aspect of our universe is the existence of ultra-hot Jupiters, a class of exoplanets that have much in common with gas giants like Jupiter but exist extremely close to their host stars — often just kilometers away from them! These planets can be so hot due to intense radiation and tidal forces from their nearby suns. Interestingly enough though, they are not all made up of pure hydrogen as one would expect for a gas giant; some have been found with significant amounts of rocky material in their cores or even solid surfaces—a peculiarity that has led scientists to nickname them \"lava planets\". These discoveries challenge our understanding and assumptions about planet formation, suggesting the flexibility of processes under extreme conditions.\n",
      "Latency: 17.931278944015503 seconds\n",
      "Tokens per second (TPS): 6.971058806807435\n"
     ]
    }
   ],
   "source": [
    "# this script gives the latency and tps data.\n",
    "\n",
    "\n",
    "import ollama\n",
    "import time\n",
    "\n",
    "def count_tokens(text):\n",
    "    # Token counting method (simplified for this example)\n",
    "    return len(text.split())\n",
    "\n",
    "# Measure latency and token usage\n",
    "start_time = time.time()\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model='phi3:3.8b', ###  phi3:3.8b , qwen2:0.5b\n",
    "    messages=[{'role': 'user', 'content': 'Tell me an interesting fact about space.'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "response_content = ''\n",
    "for chunk in stream:\n",
    "    response_content += chunk['message']['content']\n",
    "    print(chunk['message']['content'], end='', flush=True)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "num_tokens = count_tokens(response_content)\n",
    "time_taken = end_time - start_time\n",
    "tps = num_tokens / time_taken\n",
    "\n",
    "print(f\"\\nLatency: {time_taken} seconds\")\n",
    "print(f\"Tokens per second (TPS): {tps}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
